{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = util.load_openstax_course('University Physics Volume 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>learning_goal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Find the order of magnitude of the following p...</td>\n",
       "      <td>Describe the scope of physics.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Find the order of magnitude of the following p...</td>\n",
       "      <td>Calculate the order of magnitude of a quantity.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Find the order of magnitude of the following p...</td>\n",
       "      <td>Compare measurable length, mass, and timescale...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Find the order of magnitude of the following p...</td>\n",
       "      <td>Describe the relationships among models, theor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Use the orders of magnitude you found in the p...</td>\n",
       "      <td>Describe the scope of physics.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Find the order of magnitude of the following p...   \n",
       "1  Find the order of magnitude of the following p...   \n",
       "2  Find the order of magnitude of the following p...   \n",
       "3  Find the order of magnitude of the following p...   \n",
       "4  Use the orders of magnitude you found in the p...   \n",
       "\n",
       "                                       learning_goal  \n",
       "0                     Describe the scope of physics.  \n",
       "1    Calculate the order of magnitude of a quantity.  \n",
       "2  Compare measurable length, mass, and timescale...  \n",
       "3  Describe the relationships among models, theor...  \n",
       "4                     Describe the scope of physics.  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(327,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['learning_goal'].value_counts().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1036,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['question'].value_counts().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.groupby('question')['learning_goal'].agg(list).value_counts().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define what a task is \n",
    "2. (small) Debug code so that it loads Chemistry 2e\n",
    "3. (ambitious) Try a simple finetuning baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Preprcoessing data\n",
    " - (for Principles of Chemistry) stem the verb of the learning goal\n",
    " - Unicode characters:\n",
    "    - delta --> \"delta\"\n",
    "    - exponents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_shot_sample(data, learning_goal, match=True, k=5):\n",
    "  if match:\n",
    "    sample_data = data[data['learning_goal'] == learning_goal]\n",
    "  else:\n",
    "    sample_data = data[data['learning_goal'] != learning_goal]\n",
    "  return sample_data.sample(n=min(k, len(sample_data)))\n",
    "  \n",
    "  \n",
    "def meta_task(data, k=5):\n",
    "  # very clunky, but only look at data whose learning goals have enough examples\n",
    "  data = data[data['learning_goal'].isin(\n",
    "      data['learning_goal'].value_counts()[data['learning_goal'].value_counts() >= k].index\n",
    "  )]\n",
    "  query = np.random.choice(data['question'].unique())\n",
    "  learning_goal = data[data['question'] == query]['learning_goal'].sample().values[0]\n",
    "  k_shot_true = k_shot_sample(data[data['question'] != query], learning_goal, match=True, k=k)\n",
    "  k_shot_false = k_shot_sample(data[data['question'] != query], learning_goal, match=False, k=k)\n",
    "  return k_shot_true, k_shot_false, query, learning_goal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_table_of_contents(filename):\n",
    "    with open(filename) as f:\n",
    "        lines = [line.strip() for line in f]\n",
    "    chapter_names = [\n",
    "        line for line in lines \n",
    "        if re.match('[0-9]+\\.[0-9]+', line)\n",
    "    ]\n",
    "    return chapter_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "def scrape_learning_goals(url):\n",
    "    response = urllib.request.urlopen(url)\n",
    "    html = response.read().decode('utf8')\n",
    "    objectives_list = re.findall('<ul id=\\\"list-00001\\\">[\\s\\S]*?</ul>', html)\n",
    "    if len(objectives_list) == 0:\n",
    "        print('faulty:', url)\n",
    "        return []\n",
    "    learning_objectives = re.findall('<li>[\\s\\S].*</li>', objectives_list[0])\n",
    "    return [item[4:-5] for item in learning_objectives]\n",
    "\n",
    "def clean_url_extension(chapter_name):\n",
    "    return chapter_name.lower().replace(',', '').replace(':', '').replace(' ', '-').replace('.', '-')\n",
    "    \n",
    "\n",
    "def read_learning_goals(chapter_names, base_url):\n",
    "    learning_goals = {}\n",
    "    for chapter in chapter_names:\n",
    "        url = base_url + clean_url_extension(chapter)\n",
    "        print(url)\n",
    "        learning_goals[chapter] = scrape_learning_goals(url)\n",
    "    return learning_goals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://openstax.org/books/chemistry-2e/pages/1-1-chemistry-in-context\n",
      "https://openstax.org/books/chemistry-2e/pages/1-2-phases-and-classification-of-matter\n",
      "https://openstax.org/books/chemistry-2e/pages/1-3-physical-and-chemical-properties\n",
      "https://openstax.org/books/chemistry-2e/pages/1-4-measurements\n",
      "https://openstax.org/books/chemistry-2e/pages/1-5-measurement-uncertainty-accuracy-and-precision\n",
      "https://openstax.org/books/chemistry-2e/pages/1-6-mathematical-treatment-of-measurement-results\n",
      "https://openstax.org/books/chemistry-2e/pages/2-1-early-ideas-in-atomic-theory\n",
      "https://openstax.org/books/chemistry-2e/pages/2-2-evolution-of-atomic-theory\n",
      "https://openstax.org/books/chemistry-2e/pages/2-3-atomic-structure-and-symbolism\n",
      "https://openstax.org/books/chemistry-2e/pages/2-4-chemical-formulas\n",
      "https://openstax.org/books/chemistry-2e/pages/2-5-the-periodic-table\n",
      "https://openstax.org/books/chemistry-2e/pages/2-6-ionic-and-molecular-compounds\n",
      "https://openstax.org/books/chemistry-2e/pages/2-7-chemical-nomenclature\n",
      "https://openstax.org/books/chemistry-2e/pages/3-1-formula-mass-and-the-mole-concept\n",
      "https://openstax.org/books/chemistry-2e/pages/3-2-determining-empirical-and-molecular-formulas\n",
      "https://openstax.org/books/chemistry-2e/pages/3-3-molarity\n",
      "https://openstax.org/books/chemistry-2e/pages/3-4-other-units-for-solution-concentrations\n",
      "https://openstax.org/books/chemistry-2e/pages/4-1-writing-and-balancing-chemical-equations\n",
      "https://openstax.org/books/chemistry-2e/pages/4-2-classifying-chemical-reactions\n",
      "https://openstax.org/books/chemistry-2e/pages/4-3-reaction-stoichiometry\n",
      "https://openstax.org/books/chemistry-2e/pages/4-4-reaction-yields\n",
      "https://openstax.org/books/chemistry-2e/pages/4-5-quantitative-chemical-analysis\n",
      "https://openstax.org/books/chemistry-2e/pages/5-1-energy-basics\n",
      "https://openstax.org/books/chemistry-2e/pages/5-2-calorimetry\n",
      "https://openstax.org/books/chemistry-2e/pages/5-3-enthalpy\n",
      "https://openstax.org/books/chemistry-2e/pages/6-1-electromagnetic-energy\n",
      "https://openstax.org/books/chemistry-2e/pages/6-2-the-bohr-model\n",
      "https://openstax.org/books/chemistry-2e/pages/6-3-development-of-quantum-theory\n",
      "https://openstax.org/books/chemistry-2e/pages/6-4-electronic-structure-of-atoms-electron-configurations\n",
      "https://openstax.org/books/chemistry-2e/pages/6-5-periodic-variations-in-element-properties\n",
      "https://openstax.org/books/chemistry-2e/pages/7-1-ionic-bonding\n",
      "https://openstax.org/books/chemistry-2e/pages/7-2-covalent-bonding\n",
      "https://openstax.org/books/chemistry-2e/pages/7-3-lewis-symbols-and-structures\n",
      "https://openstax.org/books/chemistry-2e/pages/7-4-formal-charges-and-resonance\n",
      "https://openstax.org/books/chemistry-2e/pages/7-5-strengths-of-ionic-and-covalent-bonds\n",
      "https://openstax.org/books/chemistry-2e/pages/7-6-molecular-structure-and-polarity\n",
      "https://openstax.org/books/chemistry-2e/pages/8-1-valence-bond-theory\n",
      "https://openstax.org/books/chemistry-2e/pages/8-2-hybrid-atomic-orbitals\n",
      "https://openstax.org/books/chemistry-2e/pages/8-3-multiple-bonds\n",
      "https://openstax.org/books/chemistry-2e/pages/8-4-molecular-orbital-theory\n",
      "https://openstax.org/books/chemistry-2e/pages/9-1-gas-pressure\n",
      "https://openstax.org/books/chemistry-2e/pages/9-2-relating-pressure-volume-amount-and-temperature-the-ideal-gas-law\n",
      "https://openstax.org/books/chemistry-2e/pages/9-3-stoichiometry-of-gaseous-substances-mixtures-and-reactions\n",
      "https://openstax.org/books/chemistry-2e/pages/9-4-effusion-and-diffusion-of-gases\n",
      "https://openstax.org/books/chemistry-2e/pages/9-5-the-kinetic-molecular-theory\n",
      "https://openstax.org/books/chemistry-2e/pages/9-6-non-ideal-gas-behavior\n",
      "https://openstax.org/books/chemistry-2e/pages/10-1-intermolecular-forces\n",
      "https://openstax.org/books/chemistry-2e/pages/10-2-properties-of-liquids\n",
      "https://openstax.org/books/chemistry-2e/pages/10-3-phase-transitions\n",
      "https://openstax.org/books/chemistry-2e/pages/10-4-phase-diagrams\n",
      "https://openstax.org/books/chemistry-2e/pages/10-5-the-solid-state-of-matter\n",
      "https://openstax.org/books/chemistry-2e/pages/10-6-lattice-structures-in-crystalline-solids\n",
      "https://openstax.org/books/chemistry-2e/pages/11-1-the-dissolution-process\n",
      "https://openstax.org/books/chemistry-2e/pages/11-2-electrolytes\n",
      "https://openstax.org/books/chemistry-2e/pages/11-3-solubility\n",
      "https://openstax.org/books/chemistry-2e/pages/11-4-colligative-properties\n",
      "https://openstax.org/books/chemistry-2e/pages/11-5-colloids\n",
      "https://openstax.org/books/chemistry-2e/pages/12-1-chemical-reaction-rates\n",
      "https://openstax.org/books/chemistry-2e/pages/12-2-factors-affecting-reaction-rates\n",
      "https://openstax.org/books/chemistry-2e/pages/12-3-rate-laws\n",
      "https://openstax.org/books/chemistry-2e/pages/12-4-integrated-rate-laws\n",
      "https://openstax.org/books/chemistry-2e/pages/12-5-collision-theory\n",
      "https://openstax.org/books/chemistry-2e/pages/12-6-reaction-mechanisms\n",
      "https://openstax.org/books/chemistry-2e/pages/12-7-catalysis\n",
      "https://openstax.org/books/chemistry-2e/pages/13-1-chemical-equilibria\n",
      "https://openstax.org/books/chemistry-2e/pages/13-2-equilibrium-constants\n",
      "https://openstax.org/books/chemistry-2e/pages/13-3-shifting-equilibria-le-chateliers-principle\n",
      "https://openstax.org/books/chemistry-2e/pages/13-4-equilibrium-calculations\n",
      "https://openstax.org/books/chemistry-2e/pages/14-1-bronsted-lowry-acids-and-bases\n",
      "https://openstax.org/books/chemistry-2e/pages/14-2-ph-and-poh\n",
      "https://openstax.org/books/chemistry-2e/pages/14-3-relative-strengths-of-acids-and-bases\n",
      "https://openstax.org/books/chemistry-2e/pages/14-4-hydrolysis-of-salts\n",
      "https://openstax.org/books/chemistry-2e/pages/14-5-polyprotic-acids\n",
      "https://openstax.org/books/chemistry-2e/pages/14-6-buffers\n",
      "https://openstax.org/books/chemistry-2e/pages/14-7-acid-base-titrations\n",
      "https://openstax.org/books/chemistry-2e/pages/15-1-precipitation-and-dissolution\n",
      "https://openstax.org/books/chemistry-2e/pages/15-2-lewis-acids-and-bases\n",
      "https://openstax.org/books/chemistry-2e/pages/15-3-coupled-equilibria\n",
      "https://openstax.org/books/chemistry-2e/pages/16-1-spontaneity\n",
      "https://openstax.org/books/chemistry-2e/pages/16-2-entropy\n",
      "https://openstax.org/books/chemistry-2e/pages/16-3-the-second-and-third-laws-of-thermodynamics\n",
      "https://openstax.org/books/chemistry-2e/pages/16-4-free-energy\n",
      "https://openstax.org/books/chemistry-2e/pages/17-1-review-of-redox-chemistry\n",
      "https://openstax.org/books/chemistry-2e/pages/17-2-galvanic-cells\n",
      "https://openstax.org/books/chemistry-2e/pages/17-3-electrode-and-cell-potentials\n",
      "https://openstax.org/books/chemistry-2e/pages/17-4-potential-free-energy-and-equilibrium\n",
      "https://openstax.org/books/chemistry-2e/pages/17-5-batteries-and-fuel-cells\n",
      "https://openstax.org/books/chemistry-2e/pages/17-6-corrosion\n",
      "https://openstax.org/books/chemistry-2e/pages/17-7-electrolysis\n",
      "https://openstax.org/books/chemistry-2e/pages/18-1-periodicity\n",
      "https://openstax.org/books/chemistry-2e/pages/18-2-occurrence-and-preparation-of-the-representative-metals\n",
      "https://openstax.org/books/chemistry-2e/pages/18-3-structure-and-general-properties-of-the-metalloids\n",
      "https://openstax.org/books/chemistry-2e/pages/18-4-structure-and-general-properties-of-the-nonmetals\n",
      "https://openstax.org/books/chemistry-2e/pages/18-5-occurrence-preparation-and-compounds-of-hydrogen\n",
      "https://openstax.org/books/chemistry-2e/pages/18-6-occurrence-preparation-and-properties-of-carbonates\n",
      "https://openstax.org/books/chemistry-2e/pages/18-7-occurrence-preparation-and-properties-of-nitrogen\n",
      "https://openstax.org/books/chemistry-2e/pages/18-8-occurrence-preparation-and-properties-of-phosphorus\n",
      "https://openstax.org/books/chemistry-2e/pages/18-9-occurrence-preparation-and-compounds-of-oxygen\n",
      "https://openstax.org/books/chemistry-2e/pages/18-10-occurrence-preparation-and-properties-of-sulfur\n",
      "https://openstax.org/books/chemistry-2e/pages/18-11-occurrence-preparation-and-properties-of-halogens\n",
      "https://openstax.org/books/chemistry-2e/pages/18-12-occurrence-preparation-and-properties-of-the-noble-gases\n",
      "https://openstax.org/books/chemistry-2e/pages/19-1-occurrence-preparation-and-properties-of-transition-metals-and-their-compounds\n",
      "faulty: https://openstax.org/books/chemistry-2e/pages/19-1-occurrence-preparation-and-properties-of-transition-metals-and-their-compounds\n",
      "https://openstax.org/books/chemistry-2e/pages/19-2-coordination-chemistry-of-transition-metals\n",
      "https://openstax.org/books/chemistry-2e/pages/19-3-spectroscopic-and-magnetic-properties-of-coordination-compounds\n",
      "https://openstax.org/books/chemistry-2e/pages/20-1-hydrocarbons\n",
      "https://openstax.org/books/chemistry-2e/pages/20-2-alcohols-and-ethers\n",
      "https://openstax.org/books/chemistry-2e/pages/20-3-aldehydes-ketones-carboxylic-acids-and-esters\n",
      "https://openstax.org/books/chemistry-2e/pages/20-4-amines-and-amides\n",
      "https://openstax.org/books/chemistry-2e/pages/21-1-nuclear-structure-and-stability\n",
      "https://openstax.org/books/chemistry-2e/pages/21-2-nuclear-equations\n",
      "https://openstax.org/books/chemistry-2e/pages/21-3-radioactive-decay\n",
      "https://openstax.org/books/chemistry-2e/pages/21-4-transmutation-and-nuclear-energy\n",
      "https://openstax.org/books/chemistry-2e/pages/21-5-uses-of-radioisotopes\n",
      "https://openstax.org/books/chemistry-2e/pages/21-6-biological-effects-of-radiation\n"
     ]
    }
   ],
   "source": [
    "chapter_names = parse_table_of_contents('chemistry2e_table_of_contents.txt')\n",
    "base_url = 'https://openstax.org/books/chemistry-2e/pages/'\n",
    "\n",
    "learning_goals = read_learning_goals(chapter_names, base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('chemistry2e_subchapter_to_learning_goal.json', 'w+') as f:\n",
    "    json.dump(learning_goals, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def parse_openstax_questions_file(filename, folder_path):\n",
    "    with open(os.path.join(folder_path, filename), encoding='utf-8') as f:\n",
    "        lines = [l.strip() for l in f]\n",
    "\n",
    "    questions = {}\n",
    "    question_nums = []\n",
    "    current_subchapter = ''\n",
    "    for line in lines:\n",
    "        # when we encounter subchapter heading\n",
    "        subchapter_num = re.match('[0-9]+\\.[0-9]+', line)\n",
    "        if subchapter_num:\n",
    "            current_subchapter = subchapter_num.group(0)\n",
    "            questions[current_subchapter] = []\n",
    "            continue\n",
    "\n",
    "        # when we encounter questions\n",
    "        question_num = re.match('[0-9]+\\. ', line)\n",
    "        if question_num:\n",
    "            question_num = question_num.group(0)\n",
    "            questions[current_subchapter].append(line[len(question_num):])\n",
    "            question_nums.append(question_num)\n",
    "            continue\n",
    "\n",
    "        # if this is part of a previous question\n",
    "        questions[current_subchapter][-1] += '\\n' + line\n",
    "\n",
    "    return questions, question_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, question_nums = parse_openstax_questions_file('Chemistry2e_11.txt', 'OpenStax Dataset/Chemistry 2e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_nums = [int(q[:q.find('.')]) for q in question_nums]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_openstax_questions_folder(folder_path):\n",
    "    questions, question_nums = {}, []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('txt'):\n",
    "            q, q_num = parse_openstax_questions_file(filename, folder_path)\n",
    "            questions.update(q)\n",
    "            question_nums.extend(q_num)\n",
    "    return questions, question_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "OPENSTAX_DIR = 'OpenStax Dataset'\n",
    "\n",
    "def load_openstax_course(course_name):\n",
    "    course_code = course_name.replace(' ', '').lower()\n",
    "    with open(f'{course_code}_subchapter_to_learning_goal.json') as f:\n",
    "        subchapter_to_lgs = json.load(f)\n",
    "    \n",
    "    subchapter_to_lgs = {\n",
    "        re.findall('[0-9]+\\.[0-9]+', k)[0]: v for k, v in subchapter_to_lgs.items()\n",
    "    }\n",
    "\n",
    "    questions, question_nums = parse_openstax_questions_folder(\n",
    "        os.path.join(OPENSTAX_DIR, course_name)\n",
    "    )\n",
    "\n",
    "    dataset = []\n",
    "    for subchapter, question_list in questions.items():\n",
    "        for question in question_list:\n",
    "            for learnning_goal in subchapter_to_lgs[subchapter]:\n",
    "                dataset.append([question, learnning_goal])\n",
    "\n",
    "    dataset = pd.DataFrame(data=dataset, columns=['question', 'learning_goal'])\n",
    "    dataset['course'] = course_name\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = load_openstax_course('University Physics Volume 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "617"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d['question'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d['learning_goal'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base = 'OpenStax Dataset/Chemistry 2e'\n",
    "for filename in os.listdir(base):\n",
    "    if filename.endswith('txt'):\n",
    "        os.rename(os.path.join(base, filename), os.path.join(base, filename[len('Chemistry2e_'):]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "import pandas as pd\n",
    "\n",
    "COURSES = [\n",
    "    'Chemistry 2e', \n",
    "    'University Physics Volume 1', \n",
    "    'University Physics Volume 2', \n",
    "    'University Physics Volume 3'\n",
    "]\n",
    "\n",
    "data = pd.concat([\n",
    "    util.load_openstax_course(course) for course in COURSES\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Evaluate the net force on a current loop in an external magnetic field',\n",
       " 'Evaluate the net torque on a current loop in an external magnetic field',\n",
       " 'Define the magnetic dipole moment of a current loop']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgs = data.groupby('question').agg(list).iloc[0]['learning_goal']\n",
    "lgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amirz\\.conda\\envs\\interchange\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import openstax_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ProtoBert: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing ProtoBert from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ProtoBert from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from models.protobert import ProtoBert\n",
    "model = ProtoBert.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = openstax_dataset.get_openstax_dataloader(\n",
    "    'train',\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    100,\n",
    "    tokenizer,\n",
    ")\n",
    "\n",
    "eval_dataloader = openstax_dataset.get_openstax_dataloader(\n",
    "    'val',\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    100,\n",
    "    tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1088"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = openstax_dataset.OpenstaxDataset(num_support=2, num_query=2, tokenizer=None)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import Trainer\n",
    "\n",
    "trainer = Trainer(model, train_dataloader, eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:05, ?it/s] 0%|          | 0/1000 [00:00<?, ?it/s]\n",
      "Epoch 1 of 1000:   0%|          | 0/1000 [00:31<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 2.55 GiB already allocated; 3.05 MiB free; 2.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mfit()\n",
      "File \u001b[1;32mc:\\Users\\amirz\\Source\\smartstem\\smartstem-ai\\trainer.py:111\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_dataloader, start\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)) \u001b[39mas\u001b[39;00m train_bar:\n\u001b[0;32m    109\u001b[0m     \u001b[39mfor\u001b[39;00m batch_num, batch \u001b[39min\u001b[39;00m train_bar:\n\u001b[1;32m--> 111\u001b[0m         outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(batch)\n\u001b[0;32m    113\u001b[0m         err \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[0;32m    115\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_accumulation_steps \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    116\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss\u001b[39m.\u001b[39mreduction \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\amirz\\.conda\\envs\\interchange\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\amirz\\Source\\smartstem\\smartstem-ai\\models\\protobert.py:70\u001b[0m, in \u001b[0;36mProtoBert.forward\u001b[1;34m(self, task_batch, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m     62\u001b[0m support_representations \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbert(\n\u001b[0;32m     63\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m     64\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m     65\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[0;32m     66\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msupport\n\u001b[0;32m     67\u001b[0m )[\u001b[39m1\u001b[39m]\n\u001b[0;32m     69\u001b[0m \u001b[39m# (nq, dim)\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m query_representations \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbert(\n\u001b[0;32m     71\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m     72\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m     73\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[0;32m     74\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mquery\n\u001b[0;32m     75\u001b[0m )[\u001b[39m1\u001b[39m]\n\u001b[0;32m     77\u001b[0m \u001b[39m# (n, dim)\u001b[39;00m\n\u001b[0;32m     78\u001b[0m prototypes \u001b[39m=\u001b[39m support_representations\u001b[39m.\u001b[39mview(n, k, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\amirz\\.conda\\envs\\interchange\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\amirz\\.conda\\envs\\interchange\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1013\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1015\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m   1016\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1017\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1021\u001b[0m )\n\u001b[1;32m-> 1022\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m   1023\u001b[0m     embedding_output,\n\u001b[0;32m   1024\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   1025\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1026\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1027\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   1028\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1029\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1030\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1031\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1032\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1033\u001b[0m )\n\u001b[0;32m   1034\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1035\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\amirz\\.conda\\envs\\interchange\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\amirz\\.conda\\envs\\interchange\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:611\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    602\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    603\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    604\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    608\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    609\u001b[0m     )\n\u001b[0;32m    610\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 611\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    612\u001b[0m         hidden_states,\n\u001b[0;32m    613\u001b[0m         attention_mask,\n\u001b[0;32m    614\u001b[0m         layer_head_mask,\n\u001b[0;32m    615\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    616\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    617\u001b[0m         past_key_value,\n\u001b[0;32m    618\u001b[0m         output_attentions,\n\u001b[0;32m    619\u001b[0m     )\n\u001b[0;32m    621\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    622\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\amirz\\.conda\\envs\\interchange\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\amirz\\.conda\\envs\\interchange\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    486\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    487\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    494\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    495\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    496\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    498\u001b[0m         hidden_states,\n\u001b[0;32m    499\u001b[0m         attention_mask,\n\u001b[0;32m    500\u001b[0m         head_mask,\n\u001b[0;32m    501\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    502\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    503\u001b[0m     )\n\u001b[0;32m    504\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    506\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\amirz\\.conda\\envs\\interchange\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\amirz\\.conda\\envs\\interchange\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    418\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    419\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    425\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    426\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 427\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    428\u001b[0m         hidden_states,\n\u001b[0;32m    429\u001b[0m         attention_mask,\n\u001b[0;32m    430\u001b[0m         head_mask,\n\u001b[0;32m    431\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    432\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    433\u001b[0m         past_key_value,\n\u001b[0;32m    434\u001b[0m         output_attentions,\n\u001b[0;32m    435\u001b[0m     )\n\u001b[0;32m    436\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[0;32m    437\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\amirz\\.conda\\envs\\interchange\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\amirz\\.conda\\envs\\interchange\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:316\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    315\u001b[0m     key_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey(hidden_states))\n\u001b[1;32m--> 316\u001b[0m     value_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalue(hidden_states))\n\u001b[0;32m    318\u001b[0m query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n\u001b[0;32m    320\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_decoder:\n\u001b[0;32m    321\u001b[0m     \u001b[39m# if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[39m# Further calls to cross_attention layer can then reuse all cross-attention\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[39m# can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\u001b[39;00m\n\u001b[0;32m    327\u001b[0m     \u001b[39m# if encoder bi-directional self-attention `past_key_value` is always `None`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\amirz\\.conda\\envs\\interchange\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\amirz\\.conda\\envs\\interchange\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 2.55 GiB already allocated; 3.05 MiB free; 2.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir='outputs', evaluation_strategy='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy_score(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=openstax_dataset.OpenstaxDataset(tokenizer, 5, 5, True, 128),\n",
    "    eval_dataset=openstax_dataset.OpenstaxDataset(tokenizer, 5, 5, True, 128),\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "import pandas as pd\n",
    "\n",
    "courses = [\n",
    "    'Chemistry 2e', \n",
    "    'University Physics Volume 1', \n",
    "    'University Physics Volume 2', \n",
    "    'University Physics Volume 3'\n",
    "]\n",
    "\n",
    "data = pd.concat([\n",
    "    util.load_openstax_course(course) for course in courses\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_by_q = data.groupby('question').agg(list)\n",
    "data_by_lg = data.groupby('learning_goal').agg(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question    [In Figure 14.12, ε=12V, L=20mH, and R=5.0Ω. D...\n",
       "course      [University Physics Volume 2, University Physi...\n",
       "Name: Analyze circuits that have an inductor and resistor in series, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg_group = data_by_lg.iloc[1]\n",
    "lg_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amirz\\.conda\\envs\\interchange\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import openstax_dataset\n",
    "\n",
    "dataset = openstax_dataset.OpenstaxDataset(\n",
    "    tokenizer=None,\n",
    "    num_support=5,\n",
    "    num_query=2,\n",
    "    tokenize=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = range(10)\n",
    "b = range(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 2)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "indices = np.array(list(itertools.product(a, b)))\n",
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 4])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices[np.random.choice(indices.shape[0], replace=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from trainer import ProtoNet\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('prajjwal1/bert-tiny')\n",
    "model = BertModel.from_pretrained('prajjwal1/bert-tiny')\n",
    "\n",
    "protonet = ProtoNet(model, 0.0001, 'outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openstax_dataset import OpenstaxDataset\n",
    "\n",
    "dataset = OpenstaxDataset(num_support=5, num_query=3, tokenizer=tokenizer, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "import pandas as pd\n",
    "\n",
    "COURSES = [\n",
    "    'Chemistry 2e', \n",
    "    'University Physics Volume 1',\n",
    "    'University Physics Volume 2',\n",
    "    'University Physics Volume 3',\n",
    "]\n",
    "\n",
    "data = pd.concat([\n",
    "    util.load_openstax_course(course) for course in COURSES\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1088"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_by_lg = data.groupby('learning_goal').agg(list)\n",
    "data_by_lg.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4170"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['question'].value_counts().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openstax_dataset import OpenstaxDataset\n",
    "\n",
    "k = 5\n",
    "q = 1\n",
    "\n",
    "dataset = OpenstaxDataset(k, q, tokenizer, max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for d in dataset:\n",
    "    break\n",
    "\n",
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('learning_goal').agg(list)['question'].apply(len).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int32"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = rng.choice([1, 2, 3, 4], replace=False)\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize(x):\n",
    "    return tokenizer(\n",
    "        x,\n",
    "        return_tensors='pt',\n",
    "        max_length=128,\n",
    "        add_special_tokens=True,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "tasks = []\n",
    "num_support = 5\n",
    "num_query = 1\n",
    "\n",
    "data_by_lg = data.groupby('learning_goal').agg(list)\n",
    "data_by_question = data.groupby('question').agg(list)\n",
    "\n",
    "for learning_goal in data['learning_goal'].unique():\n",
    "    # select examples that match the sampled learning goal\n",
    "    examples_1 = data_by_lg.loc[learning_goal].question\n",
    "    support_1 = np.random.default_rng(seed=0).choice(examples_1, num_support)\n",
    "    # query_1 = np.random.default_rng(seed=0).choice(examples_1, num_query)\n",
    "\n",
    "    # select examples that do not have this learning goal\n",
    "    examples_0 = data_by_question.drop(examples_1).index\n",
    "    support_0 = np.random.default_rng(seed=0).choice(examples_0, num_support)\n",
    "    # query_0 = np.random.default_rng(seed=0).choice(examples_0, num_query)\n",
    "\n",
    "    support = list(support_0) + list(support_1)\n",
    "    query = [question]\n",
    "    labels_support = ([0] * num_support) + ([1] * num_support)\n",
    "    labels_query = [int(question in examples_1)]\n",
    "\n",
    "    if tokenizer:\n",
    "        support, query = _tokenize(support), _tokenize(query)\n",
    "    labels_support, labels_query = torch.tensor(labels_support), torch.tensor(labels_query)\n",
    "\n",
    "    tasks.append((support, labels_support, query, labels_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "489"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_length = [len(tokenizer(q)['input_ids']) for q in data['question'].unique()]\n",
    "max(tokenized_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tokenizer(question)\n",
    "len(x['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = protonet._predict(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_query = torch.stack([task[-1] for task in tasks])\n",
    "labels_query[:, 0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(107, device='cuda:0')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openstax_dataset import OpenstaxTestDataset\n",
    "\n",
    "test_dataset = OpenstaxTestDataset(\n",
    "    course_name='Chemistry 2e',\n",
    "    num_support=5,\n",
    "    num_query=5,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import dataset, sampler, dataloader\n",
    "\n",
    "class SmallDataset(dataset.Dataset):\n",
    "    def __init__(self) -> None:\n",
    "        self.rng = np.random.default_rng()\n",
    "\n",
    "        super().__init__()\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return index #self.rng.choice([1, 2, 3, 4])\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return 10\n",
    "\n",
    "\n",
    "class SmallSampler(sampler.Sampler):\n",
    "    def __init__(self, data_source=None) -> None:\n",
    "        super().__init__(data_source)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return (i for i in range(5))\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return 5\n",
    "\n",
    "dataloader = dataloader.DataLoader(\n",
    "    dataset=SmallDataset(),\n",
    "    batch_size=1,\n",
    "    sampler=SmallSampler(),\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amirz\\.conda\\envs\\cs330\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We don't have question 17 from chapter 1\n",
      "We don't have question 18 from chapter 1\n",
      "We don't have question 31 from chapter 2\n",
      "We don't have question 32 from chapter 2\n",
      "We don't have question 4 from chapter 13\n",
      "We don't have question 5 from chapter 13\n",
      "We don't have question 6 from chapter 13\n",
      "We don't have question 19 from chapter 15\n",
      "We don't have question 20 from chapter 15\n"
     ]
    }
   ],
   "source": [
    "import util\n",
    "import pandas as pd\n",
    "\n",
    "OPENSTAX_COURSES = [\n",
    "    'Chemistry 2e', \n",
    "    'University Physics Volume 1', \n",
    "    'University Physics Volume 2', \n",
    "    'University Physics Volume 3'\n",
    "]\n",
    "\n",
    "PRINCIPLES_OF_CHEMISTRY_COURSE = 'Principles of Chemistry 3rd edition'\n",
    "\n",
    "data = pd.concat([\n",
    "    util.load_openstax_course(course) for course in OPENSTAX_COURSES\n",
    "] + [util.load_principles_of_chemistry_course(PRINCIPLES_OF_CHEMISTRY_COURSE)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>learning_goal</th>\n",
       "      <th>course</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explain how you could experimentally determine...</td>\n",
       "      <td>Outline the historical development of chemistry</td>\n",
       "      <td>Chemistry 2e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Explain how you could experimentally determine...</td>\n",
       "      <td>Provide examples of the importance of chemistr...</td>\n",
       "      <td>Chemistry 2e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Explain how you could experimentally determine...</td>\n",
       "      <td>Describe the scientific method</td>\n",
       "      <td>Chemistry 2e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Explain how you could experimentally determine...</td>\n",
       "      <td>Differentiate among hypotheses, theories, and ...</td>\n",
       "      <td>Chemistry 2e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Explain how you could experimentally determine...</td>\n",
       "      <td>Provide examples illustrating macroscopic, mic...</td>\n",
       "      <td>Chemistry 2e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Explain how you could experimentally determine...   \n",
       "1  Explain how you could experimentally determine...   \n",
       "2  Explain how you could experimentally determine...   \n",
       "3  Explain how you could experimentally determine...   \n",
       "4  Explain how you could experimentally determine...   \n",
       "\n",
       "                                       learning_goal        course  \n",
       "0    Outline the historical development of chemistry  Chemistry 2e  \n",
       "1  Provide examples of the importance of chemistr...  Chemistry 2e  \n",
       "2                     Describe the scientific method  Chemistry 2e  \n",
       "3  Differentiate among hypotheses, theories, and ...  Chemistry 2e  \n",
       "4  Provide examples illustrating macroscopic, mic...  Chemistry 2e  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4875, 1267)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = data['question'].unique()\n",
    "learning_goals = data['learning_goal'].unique()\n",
    "len(questions), len(learning_goals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Outline the historical development of chemistry'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_goal = learning_goals[0]\n",
    "learning_goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "OPENAI_API_TOKEN = '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'\n",
    "openai.api_key = OPENAI_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for lg in learning_goals:\n",
    "    response = openai.Embedding.create(\n",
    "        input=lg,\n",
    "        model=\"text-similarity-curie-001\"\n",
    "    )\n",
    "    embeddings.append(response['data'][0]['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1267, 4096])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "embeddings_t = torch.tensor(embeddings)\n",
    "embeddings_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_embeddings = []\n",
    "for q in questions:\n",
    "    response = openai.Embedding.create(\n",
    "        input=q,\n",
    "        model=\"text-similarity-curie-001\"\n",
    "    )\n",
    "    question_embeddings.append(response['data'][0]['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4875, 4096])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "embeddings_q_t = torch.tensor(question_embeddings)\n",
    "embeddings_q_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(embeddings_q_t, 'question_curie_embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(embeddings_t, 'learning_goal_curie_embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('question_list.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(list(questions), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.47552"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(q.split()) for q in questions]) * 0.0200 / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26248"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(lg.split()) for lg in learning_goals]) * 0.0200 / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4875, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('question').agg(list).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>Calculate the field of a collection of source charges of either sign</th>\n",
       "      <th>Analyze circuits that have an inductor and resistor in series</th>\n",
       "      <th>Analyze complex circuits using Kirchhoff’s rules</th>\n",
       "      <th>Analyze elasticity and plasticity on a stress-strain diagram</th>\n",
       "      <th>Analyze one-dimensional and two-dimensional relative motion problems using the position and velocity vector equations.</th>\n",
       "      <th>Analyze the reason for the sparkle of diamonds</th>\n",
       "      <th>Answer qualitative questions about the effects of thermal expansion</th>\n",
       "      <th>Apply Gauss’s law in appropriate systems</th>\n",
       "      <th>Apply Gauss’s law to determine the electric field of a system with one of these symmetries</th>\n",
       "      <th>...</th>\n",
       "      <th>Writing Formulas for Ionic Compounds</th>\n",
       "      <th>Writing Hybridization and Bonding Schemes Using Valence Bond Theory</th>\n",
       "      <th>Writing Lewis Structures for Compounds Having Expanded Octets</th>\n",
       "      <th>Writing Lewis Structures for Covalent Compounds</th>\n",
       "      <th>Writing Lewis Structures for Polyatomic Ions</th>\n",
       "      <th>Writing Molecular and Empirical Formulas</th>\n",
       "      <th>Writing Nuclear Equations for Alpha Decay</th>\n",
       "      <th>Writing Nuclear Equations for Beta Decay, Positron Emission, and Electron Capture</th>\n",
       "      <th>Writing Orbital Diagrams</th>\n",
       "      <th>Writing Resonance Lewis Structures</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(a) A 200-turn circular loop of radius 50.0 cm...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(a) A 22.0-kg child is riding a playground mer...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(a) A 5.0-kg rock at a temperature of 20°C is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(a) A 5.00-kg squid initially at rest ejects 0...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(a) A cosmic ray proton moving toward Earth at...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1268 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  (a) A 200-turn circular loop of radius 50.0 cm...   \n",
       "1  (a) A 22.0-kg child is riding a playground mer...   \n",
       "2  (a) A 5.0-kg rock at a temperature of 20°C is ...   \n",
       "3  (a) A 5.00-kg squid initially at rest ejects 0...   \n",
       "4  (a) A cosmic ray proton moving toward Earth at...   \n",
       "\n",
       "    Calculate the field of a collection of source charges of either sign  \\\n",
       "0                                                  0                       \n",
       "1                                                  0                       \n",
       "2                                                  0                       \n",
       "3                                                  0                       \n",
       "4                                                  0                       \n",
       "\n",
       "   Analyze circuits that have an inductor and resistor in series  \\\n",
       "0                                                  0               \n",
       "1                                                  0               \n",
       "2                                                  0               \n",
       "3                                                  0               \n",
       "4                                                  0               \n",
       "\n",
       "   Analyze complex circuits using Kirchhoff’s rules  \\\n",
       "0                                                 0   \n",
       "1                                                 0   \n",
       "2                                                 0   \n",
       "3                                                 0   \n",
       "4                                                 0   \n",
       "\n",
       "   Analyze elasticity and plasticity on a stress-strain diagram  \\\n",
       "0                                                  0              \n",
       "1                                                  0              \n",
       "2                                                  0              \n",
       "3                                                  0              \n",
       "4                                                  0              \n",
       "\n",
       "   Analyze one-dimensional and two-dimensional relative motion problems using the position and velocity vector equations.  \\\n",
       "0                                                  0                                                                        \n",
       "1                                                  0                                                                        \n",
       "2                                                  0                                                                        \n",
       "3                                                  0                                                                        \n",
       "4                                                  0                                                                        \n",
       "\n",
       "   Analyze the reason for the sparkle of diamonds  \\\n",
       "0                                               0   \n",
       "1                                               0   \n",
       "2                                               0   \n",
       "3                                               0   \n",
       "4                                               0   \n",
       "\n",
       "   Answer qualitative questions about the effects of thermal expansion  \\\n",
       "0                                                  0                     \n",
       "1                                                  0                     \n",
       "2                                                  0                     \n",
       "3                                                  0                     \n",
       "4                                                  0                     \n",
       "\n",
       "   Apply Gauss’s law in appropriate systems  \\\n",
       "0                                         0   \n",
       "1                                         0   \n",
       "2                                         0   \n",
       "3                                         0   \n",
       "4                                         0   \n",
       "\n",
       "   Apply Gauss’s law to determine the electric field of a system with one of these symmetries  \\\n",
       "0                                                  0                                            \n",
       "1                                                  0                                            \n",
       "2                                                  0                                            \n",
       "3                                                  0                                            \n",
       "4                                                  0                                            \n",
       "\n",
       "   ...  Writing Formulas for Ionic Compounds  \\\n",
       "0  ...                                     0   \n",
       "1  ...                                     0   \n",
       "2  ...                                     0   \n",
       "3  ...                                     0   \n",
       "4  ...                                     0   \n",
       "\n",
       "   Writing Hybridization and Bonding Schemes Using Valence Bond Theory  \\\n",
       "0                                                  0                     \n",
       "1                                                  0                     \n",
       "2                                                  0                     \n",
       "3                                                  0                     \n",
       "4                                                  0                     \n",
       "\n",
       "   Writing Lewis Structures for Compounds Having Expanded Octets  \\\n",
       "0                                                  0               \n",
       "1                                                  0               \n",
       "2                                                  0               \n",
       "3                                                  0               \n",
       "4                                                  0               \n",
       "\n",
       "   Writing Lewis Structures for Covalent Compounds  \\\n",
       "0                                                0   \n",
       "1                                                0   \n",
       "2                                                0   \n",
       "3                                                0   \n",
       "4                                                0   \n",
       "\n",
       "   Writing Lewis Structures for Polyatomic Ions  \\\n",
       "0                                             0   \n",
       "1                                             0   \n",
       "2                                             0   \n",
       "3                                             0   \n",
       "4                                             0   \n",
       "\n",
       "   Writing Molecular and Empirical Formulas  \\\n",
       "0                                         0   \n",
       "1                                         0   \n",
       "2                                         0   \n",
       "3                                         0   \n",
       "4                                         0   \n",
       "\n",
       "   Writing Nuclear Equations for Alpha Decay  \\\n",
       "0                                          0   \n",
       "1                                          0   \n",
       "2                                          0   \n",
       "3                                          0   \n",
       "4                                          0   \n",
       "\n",
       "   Writing Nuclear Equations for Beta Decay, Positron Emission, and Electron Capture  \\\n",
       "0                                                  0                                   \n",
       "1                                                  0                                   \n",
       "2                                                  0                                   \n",
       "3                                                  0                                   \n",
       "4                                                  0                                   \n",
       "\n",
       "   Writing Orbital Diagrams  Writing Resonance Lewis Structures  \n",
       "0                         0                                   0  \n",
       "1                         0                                   0  \n",
       "2                         0                                   0  \n",
       "3                         0                                   0  \n",
       "4                         0                                   0  \n",
       "\n",
       "[5 rows x 1268 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummies = pd.get_dummies(\n",
    "    data, columns=['learning_goal'], prefix='', prefix_sep=''\n",
    ").groupby('question').sum(numeric_only=True).reset_index()\n",
    "\n",
    "dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(a) A 200-turn circular loop of radius 50.0 cm...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(a) A 22.0-kg child is riding a playground mer...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(a) A 5.0-kg rock at a temperature of 20°C is ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(a) A 5.00-kg squid initially at rest ejects 0...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(a) A cosmic ray proton moving toward Earth at...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  (a) A 200-turn circular loop of radius 50.0 cm...   \n",
       "1  (a) A 22.0-kg child is riding a playground mer...   \n",
       "2  (a) A 5.0-kg rock at a temperature of 20°C is ...   \n",
       "3  (a) A 5.00-kg squid initially at rest ejects 0...   \n",
       "4  (a) A cosmic ray proton moving toward Earth at...   \n",
       "\n",
       "                                              labels  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = dummies.copy()\n",
    "train_data['labels'] = train_data[train_data.columns[1:]].apply(\n",
    "    lambda l: list(l),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "train_data = train_data[train_data.columns[[0, -1]]]\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import (\n",
    "    MultiLabelClassificationModel, MultiLabelClassificationArgs\n",
    ")\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "eval_data = train_data.sample(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiLabelSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiLabelSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Optional model configuration\n",
    "model_args = MultiLabelClassificationArgs(num_train_epochs=1, output_dir='test_output')\n",
    "\n",
    "# Create a MultiLabelClassificationModel\n",
    "model = MultiLabelClassificationModel(\n",
    "    \"bert\",\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=len(train_data.iloc[0]['labels']),\n",
    "    args=model_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 2}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train_data[(train_data['labels'].apply(lambda l: set(l)) != {0, 1})].iloc[9]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amirz\\.conda\\envs\\cs330\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:612: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "  0%|          | 10/4875 [00:33<4:32:43,  3.36s/it]\n",
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_train_bert_128_0_2\n",
      "Epochs 0/1. Running Loss:    0.0261: 100%|██████████| 610/610 [03:18<00:00,  3.08it/s]\n",
      "Epoch 1 of 1: 100%|██████████| 1/1 [03:26<00:00, 206.62s/it]\n",
      "INFO:simpletransformers.classification.classification_model: Training of bert model complete. Saved to test_output.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(610, 0.10185522646566883)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train_model(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amirz\\.conda\\envs\\cs330\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 384)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "#Our sentences we like to encode\n",
    "sentences = ['This framework generates embeddings for each input sentence',\n",
    "    'Sentences are passed as a list of string.',\n",
    "    'The quick brown fox jumps over the lazy dog.']\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "#Print the embeddings\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We don't have question 17 from chapter 1\n",
      "We don't have question 18 from chapter 1\n",
      "We don't have question 31 from chapter 2\n",
      "We don't have question 32 from chapter 2\n",
      "We don't have question 4 from chapter 13\n",
      "We don't have question 5 from chapter 13\n",
      "We don't have question 6 from chapter 13\n",
      "We don't have question 19 from chapter 15\n",
      "We don't have question 20 from chapter 15\n"
     ]
    }
   ],
   "source": [
    "from openstax_dataset import nWaykShotDataset\n",
    "\n",
    "dataset = nWaykShotDataset(\n",
    "    num_support=5,\n",
    "    num_query=2,\n",
    "    tokenizer=None,\n",
    "    task_embedding_model=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "849"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amirz\\Source\\smartstem\\smartstem-ai\\openstax_dataset.py:205: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
      "  'task_embeds': torch.tensor([self.learning_goal_embeddings[index]]).repeat(2 * self.num_support, 1).unsqueeze(1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 384])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "support, _, query, _ = dataset[0]\n",
    "support['task_embeds'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 128])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "support['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('prajjwal1/bert-mini')\n",
    "\n",
    "tokens = tokenizer(\n",
    "    sentences,\n",
    "    return_tensors='pt',\n",
    "    max_length=256,\n",
    "    add_special_tokens=True,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_attention_mask=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "tokens.update({'task_emeds': torch.zeros((3, 256))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'task_emeds'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "\n",
    "lg_embeddings = torch.load('learning_goal_curie_embeddings.pt')\n",
    "q_embeddings = torch.load('question_curie_embeddings.pt')\n",
    "\n",
    "with open('learning_goal_list.txt', encoding='utf-8') as f:\n",
    "    lg_list = [line.strip() for line in f]\n",
    "\n",
    "with open('question_list.json', encoding='utf-8') as f:\n",
    "    q_list = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4875, torch.Size([4875, 4096]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(q_list), q_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1267, torch.Size([1267, 4096]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lg_embeddings), lg_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1267])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists = -torch.nn.functional.cosine_similarity(q_embeddings[:1], lg_embeddings, dim=-1)\n",
    "dists.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1267])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists = torch.cdist(q_embeddings[:1], lg_embeddings)\n",
    "dists.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1267"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances = dists.squeeze().tolist()\n",
    "len(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_to_lg = list(zip(distances, lg_list))\n",
    "\n",
    "ranked_dists = sorted(dist_to_lg, key=lambda t: t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Explain how you could experimentally determine whether the outside temperature is higher or lower than 0 °C (32 °F) without using a thermometer.'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain the relation between the intermolecular forces present within a substance and the temperatures associated with changes in its physical state\n",
      "Determine the dependence of the magnetic field from a thin, straight wire based on the distance from it and the current flowing in the wire.\n",
      "Determine the magnetic flux through a surface, knowing the strength of the magnetic field, the surface area, and the angle between the normal to the surface and the magnetic field\n",
      "Describe the processes represented by typical heating and cooling curves, and compute heat flows and enthalpy changes accompanying these processes\n",
      "Describe the effect of solute concentration on various solution properties (vapor pressure, boiling point, freezing point, and osmotic pressure)\n",
      "Determine the angular frequency, frequency, and period of a simple pendulum in terms of the length of the pendulum and the acceleration due to gravity\n",
      "Determining the Effect of a Temperature Change on Equilibrium\n",
      "Explain the differences between centripetal acceleration and tangential acceleration resulting from nonuniform circular motion.\n",
      "Determine the equilibrium separation distance between atoms in a diatomic molecule from the vibrational-rotational absorption spectrum\n",
      "Explain how the Biot-Savart law is used to determine the magnetic field due to a current in a loop of wire at a point along a line perpendicular to the plane of the loop.\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(ranked_dists[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We don't have question 17 from chapter 1\n",
      "We don't have question 18 from chapter 1\n",
      "We don't have question 31 from chapter 2\n",
      "We don't have question 32 from chapter 2\n",
      "We don't have question 4 from chapter 13\n",
      "We don't have question 5 from chapter 13\n",
      "We don't have question 6 from chapter 13\n",
      "We don't have question 19 from chapter 15\n",
      "We don't have question 20 from chapter 15\n"
     ]
    }
   ],
   "source": [
    "import util\n",
    "import pandas as pd\n",
    "\n",
    "OPENSTAX_COURSES = [\n",
    "    'Chemistry 2e', \n",
    "    'University Physics Volume 1', \n",
    "    'University Physics Volume 2', \n",
    "    'University Physics Volume 3'\n",
    "]\n",
    "\n",
    "PRINCIPLES_OF_CHEMISTRY_COURSE = 'Principles of Chemistry 3rd edition'\n",
    "\n",
    "data = pd.concat([\n",
    "    util.load_openstax_course(course) for course in OPENSTAX_COURSES\n",
    "] + [util.load_principles_of_chemistry_course(PRINCIPLES_OF_CHEMISTRY_COURSE)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>learning_goal</th>\n",
       "      <th>course</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explain how you could experimentally determine...</td>\n",
       "      <td>Outline the historical development of chemistry</td>\n",
       "      <td>Chemistry 2e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Explain how you could experimentally determine...</td>\n",
       "      <td>Provide examples of the importance of chemistr...</td>\n",
       "      <td>Chemistry 2e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Explain how you could experimentally determine...</td>\n",
       "      <td>Describe the scientific method</td>\n",
       "      <td>Chemistry 2e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Explain how you could experimentally determine...</td>\n",
       "      <td>Differentiate among hypotheses, theories, and ...</td>\n",
       "      <td>Chemistry 2e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Explain how you could experimentally determine...</td>\n",
       "      <td>Provide examples illustrating macroscopic, mic...</td>\n",
       "      <td>Chemistry 2e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Explain how you could experimentally determine...   \n",
       "1  Explain how you could experimentally determine...   \n",
       "2  Explain how you could experimentally determine...   \n",
       "3  Explain how you could experimentally determine...   \n",
       "4  Explain how you could experimentally determine...   \n",
       "\n",
       "                                       learning_goal        course  \n",
       "0    Outline the historical development of chemistry  Chemistry 2e  \n",
       "1  Provide examples of the importance of chemistr...  Chemistry 2e  \n",
       "2                     Describe the scientific method  Chemistry 2e  \n",
       "3  Differentiate among hypotheses, theories, and ...  Chemistry 2e  \n",
       "4  Provide examples illustrating macroscopic, mic...  Chemistry 2e  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['question'] == q_list[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BertModel\n",
    "from trainer import ProtoNet\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('prajjwal1/bert-tiny')\n",
    "network = BertModel.from_pretrained('prajjwal1/bert-tiny')\n",
    "\n",
    "protonet = ProtoNet(\n",
    "    network, \n",
    "    0.00001, \n",
    "    'bert_tiny_s5_q10_b4_lr1e5',\n",
    "    num_epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint iteration 800.\n"
     ]
    }
   ],
   "source": [
    "protonet.load(800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openstax_dataset import CourseTestDataset\n",
    "\n",
    "dataset = CourseTestDataset('Chemistry 2e', 5, 2, tokenizer, max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A 2.0-liter volume of hydrogen gas combined with 1.0 liter of oxygen gas to produce 2.0 liters of water vapor. Does oxygen undergo a chemical or physical change?'"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = dataset.data_by_question.iloc[12].name\n",
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_batch = dataset[123]\n",
    "labels = [l[-1].item() for l in task_batch]\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Carry out equilibrium computations involving solubility, equilibrium expressions, and solute concentrations', 'Write chemical equations and equilibrium expressions representing solubility equilibria'], dtype='object', name='learning_goal')"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "lgs = dataset.data_by_learning_goal\n",
    "lgs[np.array(labels).astype(bool)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([306])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = protonet._predict(task_batch)\n",
    "predictions.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = predictions.squeeze().cpu().numpy()\n",
    "preds.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_lgs = lgs[np.array(labels).astype(bool)].index\n",
    "pred_lgs = lgs[preds.astype(bool)].index\n",
    "\n",
    "[lg in pred_lgs for lg in true_lgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "\n",
    "def predict(task_batch, protonet):\n",
    "    with torch.no_grad():\n",
    "        predictions_batch = []\n",
    "        for task in task_batch:\n",
    "            support, labels_support, query, labels_query = task\n",
    "\n",
    "            support = {k: v.to(protonet._device) for k, v in support.items()}\n",
    "            query = {k: v.to(protonet._device) for k, v in query.items()}\n",
    "            labels_support = labels_support.to(protonet._device)\n",
    "            labels_query = labels_query.to(protonet._device)\n",
    "            n = 2\n",
    "            k = labels_support.shape[0] // n\n",
    "            # (nk, dim)\n",
    "            support_representations = protonet._network(\n",
    "                **support\n",
    "            )[1]\n",
    "\n",
    "            # (nq, dim)\n",
    "            query_representations = protonet._network(\n",
    "                **query\n",
    "            )[1]\n",
    "\n",
    "            # (n, dim)\n",
    "            prototypes = support_representations.view(n, k, -1).mean(dim=1)\n",
    "\n",
    "            # (nq, n) \n",
    "            query_distances = torch.cdist(query_representations, prototypes) \n",
    "            query_logits = F.softmax(-query_distances, dim=1)\n",
    "\n",
    "            # predictions_batch.append(torch.argmax(query_logits, dim=-1))\n",
    "            predictions_batch.append(query_logits)\n",
    "    return torch.stack(predictions_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([306, 1, 2])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = predict(task_batch, protonet)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = predictions.squeeze().cpu().tolist()\n",
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_and_lgs = list(zip(lgs.index, predictions.squeeze().cpu()[:, 1].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds_and_lgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked = sorted(preds_and_lgs, key=lambda t: t[1], reverse=True)\n",
    "\n",
    "k = 53\n",
    "top_k = [t[0] for t in ranked[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Perform stoichiometric calculations involving gaseous substances',\n",
       " 'State Dalton’s law of partial pressures and use it in calculations involving gaseous mixtures',\n",
       " 'Use the ideal gas law to compute gas densities and molar masses',\n",
       " 'Describe the supercritical fluid phase of matter',\n",
       " 'Explain the construction and use of a typical phase diagram']"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Carry out equilibrium computations involving solubility, equilibrium expressions, and solute concentrations', 'Write chemical equations and equilibrium expressions representing solubility equilibria'], dtype='object', name='learning_goal')"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_lgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lg in top_k for lg in true_lgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('cs330')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59e0f2f90842c596b98da9224b23f809feee7517a6cc086421584e6ea1d0387b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
