{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = util.load_openstax_course('University Physics Volume 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>learning_goal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Find the order of magnitude of the following p...</td>\n",
       "      <td>Describe the scope of physics.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Find the order of magnitude of the following p...</td>\n",
       "      <td>Calculate the order of magnitude of a quantity.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Find the order of magnitude of the following p...</td>\n",
       "      <td>Compare measurable length, mass, and timescale...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Find the order of magnitude of the following p...</td>\n",
       "      <td>Describe the relationships among models, theor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Use the orders of magnitude you found in the p...</td>\n",
       "      <td>Describe the scope of physics.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Find the order of magnitude of the following p...   \n",
       "1  Find the order of magnitude of the following p...   \n",
       "2  Find the order of magnitude of the following p...   \n",
       "3  Find the order of magnitude of the following p...   \n",
       "4  Use the orders of magnitude you found in the p...   \n",
       "\n",
       "                                       learning_goal  \n",
       "0                     Describe the scope of physics.  \n",
       "1    Calculate the order of magnitude of a quantity.  \n",
       "2  Compare measurable length, mass, and timescale...  \n",
       "3  Describe the relationships among models, theor...  \n",
       "4                     Describe the scope of physics.  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(327,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['learning_goal'].value_counts().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1036,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['question'].value_counts().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.groupby('question')['learning_goal'].agg(list).value_counts().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define what a task is \n",
    "2. (small) Debug code so that it loads Chemistry 2e\n",
    "3. (ambitious) Try a simple finetuning baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Preprcoessing data\n",
    " - (for Principles of Chemistry) stem the verb of the learning goal\n",
    " - Unicode characters:\n",
    "    - delta --> \"delta\"\n",
    "    - exponents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_shot_sample(data, learning_goal, match=True, k=5):\n",
    "  if match:\n",
    "    sample_data = data[data['learning_goal'] == learning_goal]\n",
    "  else:\n",
    "    sample_data = data[data['learning_goal'] != learning_goal]\n",
    "  return sample_data.sample(n=min(k, len(sample_data)))\n",
    "  \n",
    "  \n",
    "def meta_task(data, k=5):\n",
    "  # very clunky, but only look at data whose learning goals have enough examples\n",
    "  data = data[data['learning_goal'].isin(\n",
    "      data['learning_goal'].value_counts()[data['learning_goal'].value_counts() >= k].index\n",
    "  )]\n",
    "  query = np.random.choice(data['question'].unique())\n",
    "  learning_goal = data[data['question'] == query]['learning_goal'].sample().values[0]\n",
    "  k_shot_true = k_shot_sample(data[data['question'] != query], learning_goal, match=True, k=k)\n",
    "  k_shot_false = k_shot_sample(data[data['question'] != query], learning_goal, match=False, k=k)\n",
    "  return k_shot_true, k_shot_false, query, learning_goal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_table_of_contents(filename):\n",
    "    with open(filename) as f:\n",
    "        lines = [line.strip() for line in f]\n",
    "    chapter_names = [\n",
    "        line for line in lines \n",
    "        if re.match('[0-9]+\\.[0-9]+', line)\n",
    "    ]\n",
    "    return chapter_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "def scrape_learning_goals(url):\n",
    "    response = urllib.request.urlopen(url)\n",
    "    html = response.read().decode('utf8')\n",
    "    objectives_list = re.findall('<ul id=\\\"list-00001\\\">[\\s\\S]*?</ul>', html)\n",
    "    if len(objectives_list) == 0:\n",
    "        print('faulty:', url)\n",
    "        return []\n",
    "    learning_objectives = re.findall('<li>[\\s\\S].*</li>', objectives_list[0])\n",
    "    return [item[4:-5] for item in learning_objectives]\n",
    "\n",
    "def clean_url_extension(chapter_name):\n",
    "    return chapter_name.lower().replace(',', '').replace(':', '').replace(' ', '-').replace('.', '-')\n",
    "    \n",
    "\n",
    "def read_learning_goals(chapter_names, base_url):\n",
    "    learning_goals = {}\n",
    "    for chapter in chapter_names:\n",
    "        url = base_url + clean_url_extension(chapter)\n",
    "        print(url)\n",
    "        learning_goals[chapter] = scrape_learning_goals(url)\n",
    "    return learning_goals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://openstax.org/books/chemistry-2e/pages/1-1-chemistry-in-context\n",
      "https://openstax.org/books/chemistry-2e/pages/1-2-phases-and-classification-of-matter\n",
      "https://openstax.org/books/chemistry-2e/pages/1-3-physical-and-chemical-properties\n",
      "https://openstax.org/books/chemistry-2e/pages/1-4-measurements\n",
      "https://openstax.org/books/chemistry-2e/pages/1-5-measurement-uncertainty-accuracy-and-precision\n",
      "https://openstax.org/books/chemistry-2e/pages/1-6-mathematical-treatment-of-measurement-results\n",
      "https://openstax.org/books/chemistry-2e/pages/2-1-early-ideas-in-atomic-theory\n",
      "https://openstax.org/books/chemistry-2e/pages/2-2-evolution-of-atomic-theory\n",
      "https://openstax.org/books/chemistry-2e/pages/2-3-atomic-structure-and-symbolism\n",
      "https://openstax.org/books/chemistry-2e/pages/2-4-chemical-formulas\n",
      "https://openstax.org/books/chemistry-2e/pages/2-5-the-periodic-table\n",
      "https://openstax.org/books/chemistry-2e/pages/2-6-ionic-and-molecular-compounds\n",
      "https://openstax.org/books/chemistry-2e/pages/2-7-chemical-nomenclature\n",
      "https://openstax.org/books/chemistry-2e/pages/3-1-formula-mass-and-the-mole-concept\n",
      "https://openstax.org/books/chemistry-2e/pages/3-2-determining-empirical-and-molecular-formulas\n",
      "https://openstax.org/books/chemistry-2e/pages/3-3-molarity\n",
      "https://openstax.org/books/chemistry-2e/pages/3-4-other-units-for-solution-concentrations\n",
      "https://openstax.org/books/chemistry-2e/pages/4-1-writing-and-balancing-chemical-equations\n",
      "https://openstax.org/books/chemistry-2e/pages/4-2-classifying-chemical-reactions\n",
      "https://openstax.org/books/chemistry-2e/pages/4-3-reaction-stoichiometry\n",
      "https://openstax.org/books/chemistry-2e/pages/4-4-reaction-yields\n",
      "https://openstax.org/books/chemistry-2e/pages/4-5-quantitative-chemical-analysis\n",
      "https://openstax.org/books/chemistry-2e/pages/5-1-energy-basics\n",
      "https://openstax.org/books/chemistry-2e/pages/5-2-calorimetry\n",
      "https://openstax.org/books/chemistry-2e/pages/5-3-enthalpy\n",
      "https://openstax.org/books/chemistry-2e/pages/6-1-electromagnetic-energy\n",
      "https://openstax.org/books/chemistry-2e/pages/6-2-the-bohr-model\n",
      "https://openstax.org/books/chemistry-2e/pages/6-3-development-of-quantum-theory\n",
      "https://openstax.org/books/chemistry-2e/pages/6-4-electronic-structure-of-atoms-electron-configurations\n",
      "https://openstax.org/books/chemistry-2e/pages/6-5-periodic-variations-in-element-properties\n",
      "https://openstax.org/books/chemistry-2e/pages/7-1-ionic-bonding\n",
      "https://openstax.org/books/chemistry-2e/pages/7-2-covalent-bonding\n",
      "https://openstax.org/books/chemistry-2e/pages/7-3-lewis-symbols-and-structures\n",
      "https://openstax.org/books/chemistry-2e/pages/7-4-formal-charges-and-resonance\n",
      "https://openstax.org/books/chemistry-2e/pages/7-5-strengths-of-ionic-and-covalent-bonds\n",
      "https://openstax.org/books/chemistry-2e/pages/7-6-molecular-structure-and-polarity\n",
      "https://openstax.org/books/chemistry-2e/pages/8-1-valence-bond-theory\n",
      "https://openstax.org/books/chemistry-2e/pages/8-2-hybrid-atomic-orbitals\n",
      "https://openstax.org/books/chemistry-2e/pages/8-3-multiple-bonds\n",
      "https://openstax.org/books/chemistry-2e/pages/8-4-molecular-orbital-theory\n",
      "https://openstax.org/books/chemistry-2e/pages/9-1-gas-pressure\n",
      "https://openstax.org/books/chemistry-2e/pages/9-2-relating-pressure-volume-amount-and-temperature-the-ideal-gas-law\n",
      "https://openstax.org/books/chemistry-2e/pages/9-3-stoichiometry-of-gaseous-substances-mixtures-and-reactions\n",
      "https://openstax.org/books/chemistry-2e/pages/9-4-effusion-and-diffusion-of-gases\n",
      "https://openstax.org/books/chemistry-2e/pages/9-5-the-kinetic-molecular-theory\n",
      "https://openstax.org/books/chemistry-2e/pages/9-6-non-ideal-gas-behavior\n",
      "https://openstax.org/books/chemistry-2e/pages/10-1-intermolecular-forces\n",
      "https://openstax.org/books/chemistry-2e/pages/10-2-properties-of-liquids\n",
      "https://openstax.org/books/chemistry-2e/pages/10-3-phase-transitions\n",
      "https://openstax.org/books/chemistry-2e/pages/10-4-phase-diagrams\n",
      "https://openstax.org/books/chemistry-2e/pages/10-5-the-solid-state-of-matter\n",
      "https://openstax.org/books/chemistry-2e/pages/10-6-lattice-structures-in-crystalline-solids\n",
      "https://openstax.org/books/chemistry-2e/pages/11-1-the-dissolution-process\n",
      "https://openstax.org/books/chemistry-2e/pages/11-2-electrolytes\n",
      "https://openstax.org/books/chemistry-2e/pages/11-3-solubility\n",
      "https://openstax.org/books/chemistry-2e/pages/11-4-colligative-properties\n",
      "https://openstax.org/books/chemistry-2e/pages/11-5-colloids\n",
      "https://openstax.org/books/chemistry-2e/pages/12-1-chemical-reaction-rates\n",
      "https://openstax.org/books/chemistry-2e/pages/12-2-factors-affecting-reaction-rates\n",
      "https://openstax.org/books/chemistry-2e/pages/12-3-rate-laws\n",
      "https://openstax.org/books/chemistry-2e/pages/12-4-integrated-rate-laws\n",
      "https://openstax.org/books/chemistry-2e/pages/12-5-collision-theory\n",
      "https://openstax.org/books/chemistry-2e/pages/12-6-reaction-mechanisms\n",
      "https://openstax.org/books/chemistry-2e/pages/12-7-catalysis\n",
      "https://openstax.org/books/chemistry-2e/pages/13-1-chemical-equilibria\n",
      "https://openstax.org/books/chemistry-2e/pages/13-2-equilibrium-constants\n",
      "https://openstax.org/books/chemistry-2e/pages/13-3-shifting-equilibria-le-chateliers-principle\n",
      "https://openstax.org/books/chemistry-2e/pages/13-4-equilibrium-calculations\n",
      "https://openstax.org/books/chemistry-2e/pages/14-1-bronsted-lowry-acids-and-bases\n",
      "https://openstax.org/books/chemistry-2e/pages/14-2-ph-and-poh\n",
      "https://openstax.org/books/chemistry-2e/pages/14-3-relative-strengths-of-acids-and-bases\n",
      "https://openstax.org/books/chemistry-2e/pages/14-4-hydrolysis-of-salts\n",
      "https://openstax.org/books/chemistry-2e/pages/14-5-polyprotic-acids\n",
      "https://openstax.org/books/chemistry-2e/pages/14-6-buffers\n",
      "https://openstax.org/books/chemistry-2e/pages/14-7-acid-base-titrations\n",
      "https://openstax.org/books/chemistry-2e/pages/15-1-precipitation-and-dissolution\n",
      "https://openstax.org/books/chemistry-2e/pages/15-2-lewis-acids-and-bases\n",
      "https://openstax.org/books/chemistry-2e/pages/15-3-coupled-equilibria\n",
      "https://openstax.org/books/chemistry-2e/pages/16-1-spontaneity\n",
      "https://openstax.org/books/chemistry-2e/pages/16-2-entropy\n",
      "https://openstax.org/books/chemistry-2e/pages/16-3-the-second-and-third-laws-of-thermodynamics\n",
      "https://openstax.org/books/chemistry-2e/pages/16-4-free-energy\n",
      "https://openstax.org/books/chemistry-2e/pages/17-1-review-of-redox-chemistry\n",
      "https://openstax.org/books/chemistry-2e/pages/17-2-galvanic-cells\n",
      "https://openstax.org/books/chemistry-2e/pages/17-3-electrode-and-cell-potentials\n",
      "https://openstax.org/books/chemistry-2e/pages/17-4-potential-free-energy-and-equilibrium\n",
      "https://openstax.org/books/chemistry-2e/pages/17-5-batteries-and-fuel-cells\n",
      "https://openstax.org/books/chemistry-2e/pages/17-6-corrosion\n",
      "https://openstax.org/books/chemistry-2e/pages/17-7-electrolysis\n",
      "https://openstax.org/books/chemistry-2e/pages/18-1-periodicity\n",
      "https://openstax.org/books/chemistry-2e/pages/18-2-occurrence-and-preparation-of-the-representative-metals\n",
      "https://openstax.org/books/chemistry-2e/pages/18-3-structure-and-general-properties-of-the-metalloids\n",
      "https://openstax.org/books/chemistry-2e/pages/18-4-structure-and-general-properties-of-the-nonmetals\n",
      "https://openstax.org/books/chemistry-2e/pages/18-5-occurrence-preparation-and-compounds-of-hydrogen\n",
      "https://openstax.org/books/chemistry-2e/pages/18-6-occurrence-preparation-and-properties-of-carbonates\n",
      "https://openstax.org/books/chemistry-2e/pages/18-7-occurrence-preparation-and-properties-of-nitrogen\n",
      "https://openstax.org/books/chemistry-2e/pages/18-8-occurrence-preparation-and-properties-of-phosphorus\n",
      "https://openstax.org/books/chemistry-2e/pages/18-9-occurrence-preparation-and-compounds-of-oxygen\n",
      "https://openstax.org/books/chemistry-2e/pages/18-10-occurrence-preparation-and-properties-of-sulfur\n",
      "https://openstax.org/books/chemistry-2e/pages/18-11-occurrence-preparation-and-properties-of-halogens\n",
      "https://openstax.org/books/chemistry-2e/pages/18-12-occurrence-preparation-and-properties-of-the-noble-gases\n",
      "https://openstax.org/books/chemistry-2e/pages/19-1-occurrence-preparation-and-properties-of-transition-metals-and-their-compounds\n",
      "faulty: https://openstax.org/books/chemistry-2e/pages/19-1-occurrence-preparation-and-properties-of-transition-metals-and-their-compounds\n",
      "https://openstax.org/books/chemistry-2e/pages/19-2-coordination-chemistry-of-transition-metals\n",
      "https://openstax.org/books/chemistry-2e/pages/19-3-spectroscopic-and-magnetic-properties-of-coordination-compounds\n",
      "https://openstax.org/books/chemistry-2e/pages/20-1-hydrocarbons\n",
      "https://openstax.org/books/chemistry-2e/pages/20-2-alcohols-and-ethers\n",
      "https://openstax.org/books/chemistry-2e/pages/20-3-aldehydes-ketones-carboxylic-acids-and-esters\n",
      "https://openstax.org/books/chemistry-2e/pages/20-4-amines-and-amides\n",
      "https://openstax.org/books/chemistry-2e/pages/21-1-nuclear-structure-and-stability\n",
      "https://openstax.org/books/chemistry-2e/pages/21-2-nuclear-equations\n",
      "https://openstax.org/books/chemistry-2e/pages/21-3-radioactive-decay\n",
      "https://openstax.org/books/chemistry-2e/pages/21-4-transmutation-and-nuclear-energy\n",
      "https://openstax.org/books/chemistry-2e/pages/21-5-uses-of-radioisotopes\n",
      "https://openstax.org/books/chemistry-2e/pages/21-6-biological-effects-of-radiation\n"
     ]
    }
   ],
   "source": [
    "chapter_names = parse_table_of_contents('chemistry2e_table_of_contents.txt')\n",
    "base_url = 'https://openstax.org/books/chemistry-2e/pages/'\n",
    "\n",
    "learning_goals = read_learning_goals(chapter_names, base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('chemistry2e_subchapter_to_learning_goal.json', 'w+') as f:\n",
    "    json.dump(learning_goals, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def parse_openstax_questions_file(filename, folder_path):\n",
    "    with open(os.path.join(folder_path, filename), encoding='utf-8') as f:\n",
    "        lines = [l.strip() for l in f]\n",
    "\n",
    "    questions = {}\n",
    "    question_nums = []\n",
    "    current_subchapter = ''\n",
    "    for line in lines:\n",
    "        # when we encounter subchapter heading\n",
    "        subchapter_num = re.match('[0-9]+\\.[0-9]+', line)\n",
    "        if subchapter_num:\n",
    "            current_subchapter = subchapter_num.group(0)\n",
    "            questions[current_subchapter] = []\n",
    "            continue\n",
    "\n",
    "        # when we encounter questions\n",
    "        question_num = re.match('[0-9]+\\. ', line)\n",
    "        if question_num:\n",
    "            question_num = question_num.group(0)\n",
    "            questions[current_subchapter].append(line[len(question_num):])\n",
    "            question_nums.append(question_num)\n",
    "            continue\n",
    "\n",
    "        # if this is part of a previous question\n",
    "        questions[current_subchapter][-1] += '\\n' + line\n",
    "\n",
    "    return questions, question_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, question_nums = parse_openstax_questions_file('Chemistry2e_11.txt', 'OpenStax Dataset/Chemistry 2e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_nums = [int(q[:q.find('.')]) for q in question_nums]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_openstax_questions_folder(folder_path):\n",
    "    questions, question_nums = {}, []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('txt'):\n",
    "            q, q_num = parse_openstax_questions_file(filename, folder_path)\n",
    "            questions.update(q)\n",
    "            question_nums.extend(q_num)\n",
    "    return questions, question_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "OPENSTAX_DIR = 'OpenStax Dataset'\n",
    "\n",
    "def load_openstax_course(course_name):\n",
    "    course_code = course_name.replace(' ', '').lower()\n",
    "    with open(f'{course_code}_subchapter_to_learning_goal.json') as f:\n",
    "        subchapter_to_lgs = json.load(f)\n",
    "    \n",
    "    subchapter_to_lgs = {\n",
    "        re.findall('[0-9]+\\.[0-9]+', k)[0]: v for k, v in subchapter_to_lgs.items()\n",
    "    }\n",
    "\n",
    "    questions, question_nums = parse_openstax_questions_folder(\n",
    "        os.path.join(OPENSTAX_DIR, course_name)\n",
    "    )\n",
    "\n",
    "    dataset = []\n",
    "    for subchapter, question_list in questions.items():\n",
    "        for question in question_list:\n",
    "            for learnning_goal in subchapter_to_lgs[subchapter]:\n",
    "                dataset.append([question, learnning_goal])\n",
    "\n",
    "    dataset = pd.DataFrame(data=dataset, columns=['question', 'learning_goal'])\n",
    "    dataset['course'] = course_name\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = load_openstax_course('University Physics Volume 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "617"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d['question'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d['learning_goal'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base = 'OpenStax Dataset/Chemistry 2e'\n",
    "for filename in os.listdir(base):\n",
    "    if filename.endswith('txt'):\n",
    "        os.rename(os.path.join(base, filename), os.path.join(base, filename[len('Chemistry2e_'):]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "import pandas as pd\n",
    "\n",
    "COURSES = [\n",
    "    'Chemistry 2e', \n",
    "    'University Physics Volume 1', \n",
    "    'University Physics Volume 2', \n",
    "    'University Physics Volume 3'\n",
    "]\n",
    "\n",
    "data = pd.concat([\n",
    "    util.load_openstax_course(course) for course in COURSES\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Evaluate the net force on a current loop in an external magnetic field',\n",
       " 'Evaluate the net torque on a current loop in an external magnetic field',\n",
       " 'Define the magnetic dipole moment of a current loop']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgs = data.groupby('question').agg(list).iloc[0]['learning_goal']\n",
    "lgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amirz\\.conda\\envs\\interchange\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import openstax_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ProtoBert: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing ProtoBert from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ProtoBert from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from models.protobert import ProtoBert\n",
    "model = ProtoBert.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = openstax_dataset.get_openstax_dataloader(\n",
    "    'train',\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    100,\n",
    "    tokenizer,\n",
    ")\n",
    "\n",
    "eval_dataloader = openstax_dataset.get_openstax_dataloader(\n",
    "    'val',\n",
    "    4,\n",
    "    2,\n",
    "    2,\n",
    "    100,\n",
    "    tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1088"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = openstax_dataset.OpenstaxDataset(num_support=2, num_query=2, tokenizer=None)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import Trainer\n",
    "\n",
    "trainer = Trainer(model, train_dataloader, eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:05, ?it/s] 0%|          | 0/1000 [00:00<?, ?it/s]\n",
      "Epoch 1 of 1000:   0%|          | 0/1000 [00:31<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 2.55 GiB already allocated; 3.05 MiB free; 2.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mfit()\n",
      "File \u001b[1;32mc:\\Users\\amirz\\Source\\smartstem\\smartstem-ai\\trainer.py:111\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_dataloader, start\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)) \u001b[39mas\u001b[39;00m train_bar:\n\u001b[0;32m    109\u001b[0m     \u001b[39mfor\u001b[39;00m batch_num, batch \u001b[39min\u001b[39;00m train_bar:\n\u001b[1;32m--> 111\u001b[0m         outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(batch)\n\u001b[0;32m    113\u001b[0m         err \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[0;32m    115\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_accumulation_steps \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    116\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss\u001b[39m.\u001b[39mreduction \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\amirz\\.conda\\envs\\interchange\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\amirz\\Source\\smartstem\\smartstem-ai\\models\\protobert.py:70\u001b[0m, in \u001b[0;36mProtoBert.forward\u001b[1;34m(self, task_batch, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m     62\u001b[0m support_representations \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbert(\n\u001b[0;32m     63\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m     64\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m     65\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[0;32m     66\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msupport\n\u001b[0;32m     67\u001b[0m )[\u001b[39m1\u001b[39m]\n\u001b[0;32m     69\u001b[0m \u001b[39m# (nq, dim)\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m query_representations \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbert(\n\u001b[0;32m     71\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m     72\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m     73\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[0;32m     74\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mquery\n\u001b[0;32m     75\u001b[0m )[\u001b[39m1\u001b[39m]\n\u001b[0;32m     77\u001b[0m \u001b[39m# (n, dim)\u001b[39;00m\n\u001b[0;32m     78\u001b[0m prototypes \u001b[39m=\u001b[39m support_representations\u001b[39m.\u001b[39mview(n, k, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\amirz\\.conda\\envs\\interchange\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\amirz\\.conda\\envs\\interchange\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1013\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1015\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m   1016\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1017\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1021\u001b[0m )\n\u001b[1;32m-> 1022\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m   1023\u001b[0m     embedding_output,\n\u001b[0;32m   1024\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   1025\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1026\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1027\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   1028\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1029\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1030\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1031\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1032\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1033\u001b[0m )\n\u001b[0;32m   1034\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1035\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\amirz\\.conda\\envs\\interchange\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\amirz\\.conda\\envs\\interchange\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:611\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    602\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    603\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    604\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    608\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    609\u001b[0m     )\n\u001b[0;32m    610\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 611\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    612\u001b[0m         hidden_states,\n\u001b[0;32m    613\u001b[0m         attention_mask,\n\u001b[0;32m    614\u001b[0m         layer_head_mask,\n\u001b[0;32m    615\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    616\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    617\u001b[0m         past_key_value,\n\u001b[0;32m    618\u001b[0m         output_attentions,\n\u001b[0;32m    619\u001b[0m     )\n\u001b[0;32m    621\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    622\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\amirz\\.conda\\envs\\interchange\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\amirz\\.conda\\envs\\interchange\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    486\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    487\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    494\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    495\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    496\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    498\u001b[0m         hidden_states,\n\u001b[0;32m    499\u001b[0m         attention_mask,\n\u001b[0;32m    500\u001b[0m         head_mask,\n\u001b[0;32m    501\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    502\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    503\u001b[0m     )\n\u001b[0;32m    504\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    506\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\amirz\\.conda\\envs\\interchange\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\amirz\\.conda\\envs\\interchange\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    418\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    419\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    425\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    426\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 427\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    428\u001b[0m         hidden_states,\n\u001b[0;32m    429\u001b[0m         attention_mask,\n\u001b[0;32m    430\u001b[0m         head_mask,\n\u001b[0;32m    431\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    432\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    433\u001b[0m         past_key_value,\n\u001b[0;32m    434\u001b[0m         output_attentions,\n\u001b[0;32m    435\u001b[0m     )\n\u001b[0;32m    436\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[0;32m    437\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\amirz\\.conda\\envs\\interchange\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\amirz\\.conda\\envs\\interchange\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:316\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    315\u001b[0m     key_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey(hidden_states))\n\u001b[1;32m--> 316\u001b[0m     value_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalue(hidden_states))\n\u001b[0;32m    318\u001b[0m query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n\u001b[0;32m    320\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_decoder:\n\u001b[0;32m    321\u001b[0m     \u001b[39m# if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[39m# Further calls to cross_attention layer can then reuse all cross-attention\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[39m# can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\u001b[39;00m\n\u001b[0;32m    327\u001b[0m     \u001b[39m# if encoder bi-directional self-attention `past_key_value` is always `None`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\amirz\\.conda\\envs\\interchange\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\amirz\\.conda\\envs\\interchange\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 2.55 GiB already allocated; 3.05 MiB free; 2.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir='outputs', evaluation_strategy='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy_score(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=openstax_dataset.OpenstaxDataset(tokenizer, 5, 5, True, 128),\n",
    "    eval_dataset=openstax_dataset.OpenstaxDataset(tokenizer, 5, 5, True, 128),\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "import pandas as pd\n",
    "\n",
    "courses = [\n",
    "    'Chemistry 2e', \n",
    "    'University Physics Volume 1', \n",
    "    'University Physics Volume 2', \n",
    "    'University Physics Volume 3'\n",
    "]\n",
    "\n",
    "data = pd.concat([\n",
    "    util.load_openstax_course(course) for course in courses\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_by_q = data.groupby('question').agg(list)\n",
    "data_by_lg = data.groupby('learning_goal').agg(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question    [In Figure 14.12, ε=12V, L=20mH, and R=5.0Ω. D...\n",
       "course      [University Physics Volume 2, University Physi...\n",
       "Name: Analyze circuits that have an inductor and resistor in series, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg_group = data_by_lg.iloc[1]\n",
    "lg_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amirz\\.conda\\envs\\interchange\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import openstax_dataset\n",
    "\n",
    "dataset = openstax_dataset.OpenstaxDataset(\n",
    "    tokenizer=None,\n",
    "    num_support=5,\n",
    "    num_query=2,\n",
    "    tokenize=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = range(10)\n",
    "b = range(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 2)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "indices = np.array(list(itertools.product(a, b)))\n",
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 4])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices[np.random.choice(indices.shape[0], replace=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('interchange')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d283f86b5d2fbc31f35f78f73be4d0bb5be67dfbb54fdd34f287f29d60f844e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
